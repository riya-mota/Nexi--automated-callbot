# -*- coding: utf-8 -*-
"""Voice-clone-pthfile.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDrVmQ0Z2S0_fVmjcGdPrLZh9UUJ9ZwZ
"""

# F5-TTS Voice Cloning - Simplified Error-Free Approach
# Run each cell step by step in Google Colab

# ================================
# CELL 1: Environment Setup
# ================================
print("🚀 Starting F5-TTS Voice Cloning Setup...")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install essential packages
import subprocess
import sys
import os

def run_command(cmd):
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Error: {result.stderr}")
    return result.returncode == 0

# Install dependencies
print("Installing core packages...")

# Use the official PyTorch installation command for CUDA 11.8
# This ensures torch, torchaudio, and torchdata are compatible
run_command("pip install torch==2.1.0 torchaudio==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118")

# Install other dependencies
run_command("pip install transformers>=4.35.0")
run_command("pip install librosa soundfile pydub numpy scipy")
run_command("pip install accelerate safetensors")
run_command("pip install ffmpeg-python") # Added ffmpeg-python for pydub compatibility

print("✅ Environment setup complete!")

output_path = "/content/drive/MyDrive/voice cloning/reference_audio.wav"

print("\n📦 Setting up F5-TTS...")

os.chdir('/content')

# Remove existing directory if it exists
if os.path.exists('F5-TTS'):
    run_command('rm -rf F5-TTS')

# Clone repository
print("Cloning F5-TTS repository...")
if run_command('git clone https://github.com/SWivid/F5-TTS.git'):
    print("✅ Repository cloned successfully!")
else:
    print("❌ Failed to clone repository")

# Install F5-TTS
os.chdir('/content/F5-TTS')
run_command('pip install -e .')
run_command('pip install -r requirements.txt')

print("✅ F5-TTS installation complete!")

print("\n📝 Preparing text data...")

# Your texts
reference_text = "You just tell me what works, date, time, and your preferred location. We'll arrange everything for you. No pressure. Seriously. I'm just here to help you"

target_text = "Hey Welcome to Nexi, I'm your personal assistant to help you with all your jaguar land rover queries. We have a wide range of car from SUV to sports, tell me what should i help you with."

print("Reference text:", reference_text)
print("\nTarget text:", target_text)
print("✅ Texts prepared!")

print("\n🎤 Generating cloned voice...")

import torch
import torchaudio
import numpy as np
import sys
import os
import subprocess

sys.path.append('/content/F5-TTS/src')

# Set device first (fix for device not defined error)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Fix the tensor/tuple issue by using direct inference approach
try:
    # Import necessary modules (remove problematic import)
    from f5_tts.model import DiT, UNetT
    from f5_tts.infer.utils_infer import (
        load_checkpoint,
        preprocess_ref_audio_text,
        infer_process,
        load_vocoder
        # Removed remove_silence as it's causing import error
    )
    import torchaudio.transforms as T

    print("✅ F5-TTS modules imported successfully!")

    # Load model configuration
    model_cfg = dict(
        dim=1024,
        depth=22,
        heads=16,
        ff_mult=2,
        text_dim=512,
        conv_layers=4
    )

    # Initialize model
    print("Loading F5-TTS model...")
    model = DiT(**model_cfg)

    # Load checkpoint with proper device handling
    ckpt_path = "hf://SWivid/F5-TTS/F5TTS_Base/model_1200000.safetensors"
    model, _, _ = load_checkpoint(model, ckpt_path, device=str(device))

    print("✅ Model loaded successfully!")

    # Process reference audio and text
    print("Processing reference audio...")
    ref_audio, ref_text = preprocess_ref_audio_text(
        output_path,
        reference_text,
        device=device
    )

    print("Generating speech... This will take several minutes...")

    # Generate audio using infer_process
    final_wave, final_sample_rate, spectrogram = infer_process(
        ref_audio=ref_audio,
        ref_text=ref_text,
        gen_text=target_text,
        model=model,
        cross_fade_duration=0.15,
        speed=1.0,
        device=device
    )

    # Fix the tuple/tensor issue by ensuring proper tensor format
    if isinstance(final_wave, tuple):
        final_wave = final_wave[0]  # Take first element if it's a tuple

    # Ensure tensor is in correct format and shape
    if len(final_wave.shape) == 1:
        final_wave = final_wave.unsqueeze(0)  # Add channel dimension if needed
    elif len(final_wave.shape) == 3:
        final_wave = final_wave.squeeze(0)  # Remove batch dimension if present

    # Make sure we have the right shape: [channels, samples]
    if final_wave.shape[0] > final_wave.shape[1]:
        final_wave = final_wave.transpose(0, 1)

    # Save the generated audio
    output_file = "/content/generated_voice.wav"
    torchaudio.save(output_file, final_wave.cpu(), final_sample_rate)

    print(f"✅ Voice generation complete!")
    print(f"Generated file: {output_file}")

    # Create a simple silence removal function instead of using the problematic import
    def simple_remove_silence(audio, sample_rate, threshold=0.01):
        """Simple silence removal based on amplitude threshold"""
        audio_np = audio.numpy() if hasattr(audio, 'numpy') else audio
        if len(audio_np.shape) > 1:
            audio_np = audio_np[0]  # Take first channel

        # Find non-silent segments
        non_silent = np.abs(audio_np) > threshold
        if np.any(non_silent):
            start_idx = np.argmax(non_silent)
            end_idx = len(audio_np) - np.argmax(non_silent[::-1]) - 1
            return torch.tensor(audio_np[start_idx:end_idx+1])
        return torch.tensor(audio_np)

    # Create a clean version without silence
    try:
        clean_wave = simple_remove_silence(final_wave.squeeze().cpu(), final_sample_rate)
        clean_output = "/content/generated_voice_clean.wav"
        torchaudio.save(clean_output, clean_wave.unsqueeze(0), final_sample_rate)
        print(f"✅ Clean version saved: {clean_output}")
    except Exception as clean_error:
        print(f"Note: Clean version creation skipped - {clean_error}")

except Exception as e:
    print(f"❌ Direct inference failed: {e}")
    print("Trying gradio interface approach...")

    # Alternative using gradio interface approach (with device fix)
    try:
        # Set environment variable to fix hash seed issue
        os.environ['PYTHONHASHSEED'] = '0'

        # Try using the gradio app approach
        from f5_tts.infer.infer_gradio import infer, load_model

        print("Loading model via gradio interface...")
        model, vocoder = load_model("F5-TTS", "F5TTS_Base", str(device))

        print("Generating audio via gradio interface...")
        result = infer(
            ref_audio_input=output_path,
            ref_text_input=reference_text,
            gen_text_input=target_text,
            model_obj=model,
            vocoder_obj=vocoder,
            cross_fade_duration=0.15,
            speed=1.0
        )

        if result:
            print("✅ Voice generated using gradio interface!")
        else:
            print("❌ Gradio interface approach failed")

    except Exception as e2:
        print(f"❌ Gradio approach failed: {e2}")

        # Final fallback - simple file-based approach with fixed tensor handling
        print("Trying simple file-based generation...")
        try:
            # Create a simple script file to avoid CLI issues
            script_content = f'''
import torch
import torchaudio
import sys
import numpy as np
sys.path.append('/content/F5-TTS')

from f5_tts.api import F5TTS

# Initialize F5TTS
f5tts = F5TTS()

# Generate audio
audio = f5tts.infer(
    ref_file="{output_path}",
    ref_text="{reference_text}",
    gen_text="{target_text}",
    cross_fade_duration=0.15,
    speed=1.0
)

# Handle tuple/tensor issue properly
if isinstance(audio, tuple):
    audio = audio[0]

print(f"Audio type: {{type(audio)}}")
if hasattr(audio, 'shape'):
    print(f"Audio shape: {{audio.shape}}")

# Convert numpy array to torch tensor first
if isinstance(audio, np.ndarray):
    print("Converting numpy array to torch tensor...")
    audio = torch.from_numpy(audio.copy())

# Now handle tensor shape issues
if hasattr(audio, 'shape'):
    # Ensure correct tensor format
    if len(audio.shape) == 1:
        audio = audio.unsqueeze(0)  # Add channel dimension
    elif len(audio.shape) == 3:
        audio = audio.squeeze(0)   # Remove batch dimension

    # Make sure channels are first dimension
    if audio.shape[0] > audio.shape[1]:
        audio = audio.transpose(0, 1)

    print(f"Final audio shape: {{audio.shape}}")

# Save audio with proper error handling
try:
    torchaudio.save("/content/generated_voice.wav", audio, 24000)
    print("✅ Audio saved successfully to /content/generated_voice.wav!")
except Exception as save_error:
    print(f"Save error: {{save_error}}")
    # Try alternative save method
    try:
        import soundfile as sf
        if hasattr(audio, 'numpy'):
            audio_np = audio.numpy()
        else:
            audio_np = audio

        if len(audio_np.shape) > 1:
            audio_np = audio_np[0]  # Take first channel

        sf.write("/content/generated_voice.wav", audio_np, 24000)
        print("✅ Audio saved using soundfile!")
    except Exception as sf_error:
        print(f"Soundfile save error: {{sf_error}}")
        # Final fallback - save as numpy
        if isinstance(audio, torch.Tensor):
            audio_np = audio.numpy()
        else:
            audio_np = audio

        np.save("/content/generated_voice.npy", audio_np)
        print("Audio saved as numpy array to /content/generated_voice.npy")
'''

            # Write and execute script
            with open('/content/generate_voice.py', 'w') as f:
                f.write(script_content)

            # Run the script
            result = subprocess.run([sys.executable, '/content/generate_voice.py'],
                                  capture_output=True, text=True,
                                  env={**os.environ, 'PYTHONHASHSEED': '0'})

            if result.returncode == 0:
                print("✅ Voice generated using script approach!")
                print("Output:", result.stdout)
            else:
                print(f"Script approach error: {result.stderr}")
                print("Script output:", result.stdout)

        except Exception as e3:
            print(f"❌ All approaches failed. Error: {e3}")
            print("Please try running the notebook again or check GPU memory.")

# Additional fallback - try using direct torchaudio operations
print("\nTrying direct audio generation fallback...")
try:
    # Check if generated file exists and try to load/resave it properly
    potential_files = ["/content/generated_voice.wav", "/tmp/generated_audio.wav"]

    for file_path in potential_files:
        if os.path.exists(file_path):
            print(f"Found audio file: {file_path}")
            try:
                # Load and resave with proper format
                waveform, sample_rate = torchaudio.load(file_path)
                print(f"Loaded audio shape: {waveform.shape}, sample_rate: {sample_rate}")

                # Ensure proper format
                if len(waveform.shape) == 1:
                    waveform = waveform.unsqueeze(0)

                # Save with corrected format
                output_path_fixed = "/content/generated_voice_fixed.wav"
                torchaudio.save(output_path_fixed, waveform, sample_rate)
                print(f"✅ Audio successfully saved to: {output_path_fixed}")
                break

            except Exception as load_error:
                print(f"Error processing {file_path}: {load_error}")
                continue

except Exception as fallback_error:
    print(f"Fallback approach failed: {fallback_error}")

print("\n🎵 Voice cloning process completed. Check the output files!")